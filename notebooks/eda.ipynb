{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8be0e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().resolve().parents[0]\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c61a617-4281-4630-b530-9880798fce41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\jacka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jacka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\jacka/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jacka/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', download_dir=os.path.expanduser('~/nltk_data'))\n",
    "nltk.download('stopwords', download_dir=os.path.expanduser('~/nltk_data'))\n",
    "nltk.download('vader_lexicon', download_dir=os.path.expanduser('~/nltk_data'))\n",
    "nltk.download('punkt_tab', download_dir=os.path.expanduser('~/nltk_data'))\n",
    "\n",
    "from src.data_loader import find_data_dir, load_data, clean_html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfadce8a",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fcbf8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data_dir = find_data_dir()\n",
    "csv_path = os.path.join(data_dir, \"raw\", \"IMDB Dataset.csv\")\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581745d9",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf8be97",
   "metadata": {},
   "source": [
    "#### Look at the dataset structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6e2b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae4bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23589143",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089473b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597c0ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty or whitespace-only reviews\n",
    "df['empty_review'] = df['review'].str.strip().eq('')\n",
    "df['empty_review'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b595e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate reviews\n",
    "df.duplicated(subset='review').sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8c7465",
   "metadata": {},
   "source": [
    "Observation - It is a relatively balance dataset with total 50K rows split evenly between positive and negative reviews (25K each). There are no null values and empty reviews, but there are ~420 duplicate reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ceec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['char_count'] = df['review'].str.len()\n",
    "df['word_count'] = df['review'].apply(lambda x: len(x.split()))\n",
    "df[['char_count', 'word_count']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91837c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df[['char_count', 'word_count']]\n",
    " .groupby(df['sentiment'])\n",
    " .describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e710471",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.boxplot(x='sentiment', y='word_count', data=df)\n",
    "plt.title(\"Word Count by Sentiment\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60215c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.histplot(df,x='word_count', bins=50, \n",
    "             hue='sentiment', element=\"step\",\n",
    "             multiple=\"dodge\",\n",
    "             )\n",
    "plt.title(\"Word Count Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0aa378",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_review'] = df['review'].apply(clean_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe400bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def get_tokens(text):\n",
    "#     return word_tokenize(text.lower())\n",
    "\n",
    "# # tokens = df['clean_review'].apply(get_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e26825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f48ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#N grams analysis\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    ngram_range=(2,2),\n",
    "    min_df=5,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "X_bigrams = vectorizer.fit_transform(df['clean_review'])\n",
    "\n",
    "bigram_freq = np.asarray(X_bigrams.sum(axis=0)).flatten()\n",
    "bigrams = vectorizer.get_feature_names_out()\n",
    "\n",
    "top_bigrams = sorted(\n",
    "    zip(bigrams, bigram_freq),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:20]\n",
    "\n",
    "top_bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db3b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    min_df=5,\n",
    "    max_df=0.9,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "X_tfidf = tfidf.fit_transform(df['clean_review'])\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "idf_scores = tfidf.idf_\n",
    "\n",
    "top_tfidf = sorted(\n",
    "    zip(feature_names, idf_scores),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:20]\n",
    "\n",
    "top_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9087b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train, validation, test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[\"clean_review\"]\n",
    "y = (df[\"sentiment\"]==\"positive\").astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9049c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "data_dir = find_data_dir()\n",
    "processed_path = os.path.join(data_dir, \"processed\")\n",
    "\n",
    "# Save data using joblib\n",
    "joblib.dump(X_train, os.path.join(processed_path, \"X_train.pkl\"))\n",
    "joblib.dump(X_test, os.path.join(processed_path, \"X_test.pkl\"))\n",
    "joblib.dump(y_train, os.path.join(processed_path, \"y_train.pkl\"))\n",
    "joblib.dump(y_test, os.path.join(processed_path, \"y_test.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e3f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf  = tfidf.transform(X_test)\n",
    "print(f\"TF-IDF Train shape: {X_train_tfidf.shape}\")\n",
    "print(f\"TF-IDF Test shape: {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d4c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8d0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "y_pred = lr.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae92abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e31d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names_out()\n",
    "coef = lr.coef_[0]\n",
    "\n",
    "top_pos = sorted(zip(coef, feature_names), reverse=True)[:20]\n",
    "top_neg = sorted(zip(coef, feature_names))[:20]\n",
    "\n",
    "top_pos, top_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc4c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"C\": [0.01, 0.1, 1, 10],\n",
    "    \"penalty\": [\"l2\"]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    param_grid,\n",
    "    scoring=\"f1\",\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train_tfidf, y_train)\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "best_lr = grid.best_estimator_\n",
    "accuracy_score(y_test, best_lr.predict(X_test_tfidf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b7c5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348093c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e484f3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a0502c8",
   "metadata": {},
   "source": [
    "Tree Based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377ca70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_tree = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1,2),\n",
    "    min_df=10,\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "X_train_tree = tfidf_tree.fit_transform(X_train)\n",
    "X_test_tree  = tfidf_tree.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18f3aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=20,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf.fit(X_train_tree, y_train)\n",
    "y_pred_rf = rf.predict(X_test_tree)\n",
    "print(\"Random Forest Test Accuracy:\", accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e69cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "xgb.fit(X_train_tree, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test_tree)\n",
    "print(\"XGBoost Test Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a5af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_xgb = TfidfVectorizer(\n",
    "    max_features=3000,\n",
    "    ngram_range=(1,1),\n",
    "    min_df=10,\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "X_train_xgb = tfidf_xgb.fit_transform(X_train)\n",
    "X_test_xgb  = tfidf_xgb.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9e78ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    n_jobs=-1,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "param_dist = {\n",
    "    \"n_estimators\": [100, 150, 200],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"subsample\": [0.7, 0.8, 0.9],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"min_child_weight\": [1, 3, 5],\n",
    "    \"reg_alpha\": [0, 0.1, 1],\n",
    "    \"reg_lambda\": [1, 5, 10]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=15,\n",
    "    scoring=\"f1\",\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_xgb, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa83018",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb_rs = random_search.predict(X_test_xgb)\n",
    "print(\"XGBoost Test Accuracy:\", accuracy_score(y_test, y_pred_xgb_rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e145c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import joblib  \n",
    "\n",
    "\n",
    "repo_root = os.path.dirname(find_data_dir())  # go up from 'data' to repo root\n",
    "models_dir = os.path.join(repo_root, \"models\")\n",
    "\n",
    "# Create models folder if it doesn't exist\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save models using joblib\n",
    "joblib.dump(best_lr, os.path.join(models_dir, \"logistic_regression.pkl\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
